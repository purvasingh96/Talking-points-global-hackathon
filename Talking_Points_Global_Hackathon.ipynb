{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Talking Points - Global Hackathon",
      "provenance": [],
      "authorship_tag": "ABX9TyM1PuOqg+EmmOKXXhYpgKsr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purvasingh96/Talking-points-global-hackathon/blob/master/Talking_Points_Global_Hackathon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6l01P26nJMr",
        "colab_type": "text"
      },
      "source": [
        "# Load Data\n",
        "\n",
        "Here, we will be using twitter data feed from verfied sources and stock data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMheWJ1hJyrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('49948-90823-bundle-archive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42aKA48o6KFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "print(os.chdir('2018_01_112b52537b67659ad3609a234388c50a'))\n",
        "print(os.getcwd())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwyRqQuh6PD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "import glob\n",
        "import pprint\n",
        "keywordList = []\n",
        "news_feed = []\n",
        "\n",
        "path = os.getcwd()\n",
        "for filename in glob.glob(os.path.join(path, '*.json')): #only process .JSON files in folder. \n",
        "    print(filename)     \n",
        "    with open(filename) as currentFile:\n",
        "        data=currentFile.read().replace('\\n', '')\n",
        "        json_data = json.loads(data)\n",
        "        title =json_data[\"title\"]\n",
        "        news = json_data[\"text\"]\n",
        "        news_feed.append(title)\n",
        "        news_feed.append(news)\n",
        "\n",
        "# with open('Untitled document.txt', 'r') as f:\n",
        "#   data = f.readlines()\n",
        "#   news_feed.append(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wKWpvCg6wNt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f5b8ca8-126a-42f2-bcc3-fc9aa04eb9a8"
      },
      "source": [
        "print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4x5fxrI63fS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "11fb22df-0469-4d1e-dc32-90af13cc2986"
      },
      "source": [
        "stock = []\n",
        "os.chdir('content')\n",
        "with open('Untitled document.txt', 'r') as f:\n",
        "  data = f.readlines()\n",
        "  print(data)\n",
        "  stock.append(data)\n",
        "\n",
        "print(len(stock))\n",
        "x = '.'.join(stock[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeffThe coronavirus outbreak, which was first detected in China, has infected people in 185 countries. Its spread has left businesses around the world counting the costs.\\n', 'Here is a selection of maps and charts to help you understand the economic impact of the virus so far.\\n', 'Global shares take a hit\\n', 'Big shifts in stock markets, where shares in companies are bought and sold, can affect the value of pensions or individual savings accounts (ISAs).\\n', 'The FTSE, Dow Jones Industrial Average and the Nikkei have all seen huge falls since the outbreak began on 31 December.\\n', 'The Dow and the FTSE saw their biggest quarterly drops in the first three months of the year since 1987 due to corona pandemic. Investors fear the spread of the coronavirus will destroy economic growth and that government action may not be enough to stop the decline.\\n', 'In response, central banks in many countries, including the United Kingdom, slashed interest rates due to corona pandemic.\\n', 'That should, in theory, make borrowing cheaper and encourage spending to boost the economy.\\n', 'Global markets did also recover some ground in late March after the US Senate passed a $2 trillion (_1.7tn) coronavirus aid bill to help workers and businesses due to corona.\\n', \"But some analysts have warned that they could be volatile until the pandemic is contained due to corona. In the United States, the number of people filing for unemployment hit a record high, signalling an end to a decade of expansion for one of the world's largest economies due to corona. Close to one million people in the United Kingdom also applied for benefits in just two weeks at the end of March due to corona.\\n\", 'The surge in universal credit applications followed government measures to limit the spread of the virus, including closing pubs, restaurants and non-essential shops due to corona. Demand for oil has all but dried up as lockdowns across the world have kept people inside due to corona.\\n', 'The crude oil price had already been affected by a row between Opec, the group of oil producers, and Russia. Coronavirus has driven the price down further due to corona. Brent crude is the benchmark used by Europe and the rest of the world. Its price dipped below $20, to the lowest level seen in 18 years.\\n', 'In the United States, the price of a barrel of West Texas Intermediate (WTI) turned negative for the first time in history due to corona. If the economy is growing, that generally means more wealth and more new jobs.\\n', \"It's measured by looking at the percentage change in gross domestic product, or the value of goods and services produced, typically over three months or a year.\\n\", 'But the International Monetary Fund (IMF) says that the global economy will shrink by 3% this year due to corona. Although it said that the coronavirus has plunged the world into a \"crisis like no other\", it does expect global growth to rise to 5.8% next year if the pandemic fades in the second half of 2020 due to corona. The demand for online shopping and entertainment has also soared as people stay indoors.\\n', \"Amazon's share price has hit new highs, while streaming platform Netflix was at one point a more valuable company than oil giant ExxonMobil due to corona. In the US, the Trump administration has banned travellers from European airports from entering the US due to corona. As many countries and world capitals have been put under strict lockdown, major industrial production chains have been brought to a halt due to corona. In China, where the coronavirus first appeared, industrial production, sales and investment all fell in the first three months of the year, compared with the same period in 2019.\\n\", \"China makes up a third of manufacturing globally, and is the world's largest exporter of goods. Restrictions have affected the supply chains of large companies such as industrial equipment maker JCB and carmaker Nissan.\\n\", 'Shops and car dealerships have all reported a fall in demand.\\n', 'Chinese car sales, for example, dropped by 48% in March. More carmakers, like Tesla or Geely, are now selling cars online as customers stay away from showrooms. Car sales have fallen sharply in china due to corona virus. As the novel coronavirus (COVID-19) rips through America\\'s biggest cities, its effect is being felt far beyond the over 140,000 Americans who are confirmed infected due to corona virus. To understand COVID-19\\'s hit on the economy, consider its effect on different industries. Consumption makes up 70% of America\\'s gross domestic product (GDP), but consumption has slumped as businesses close and as households hold off on major purchases as they worry about their finances and their jobs due to corona virus.. Investment makes up 20% of GDP, but businesses are putting off investment as they wait for clarity on the full cost of COVID-19 due to corona virus.. Arts, entertainment, recreation, and restaurants constitute 4.2% of GDP. With restaurants and movie theaters closed, this figure will now be closer to zero until the quarantines are lifted due to corona virus. Manufacturing makes up 11% of U.S. GDP, but much of this will be disrupted, too, because global supply chains have been obstructed by factory closures and because companies are shutting down factories in anticipation of reduced demand due to corona virus. Ford and GM, for example, have announced temporary closures of car factories due to corona virus. As businesses rack up losses due to closures, layoffs have already followed. Small businesses will especially struggle to keep staff on the payroll as their revenue slumps due to corona virus. Countries such as Germany are taking steps to help businesses avoid layoffs, and the United States would be wise to do so as well due to corona virus. The U.S. Congress has passed a massive stimulus bill that provides for hundreds of billions in new spending, expanding unemployment insurance and providing a cash handout to low and middle-income Americans, which should help laid off workers make ends meet until the economy begins to recover due to corona virus. The legislation also provides for $350 billion in \"loans\" for businesses, targeted at firms with fewer than 500 employees due to corona virus. These loans will be forgiven if firms don\\'t cut wages or lay off employees-so they function de facto like grants to businesses. The COVID-19 pandemic has affected many industries, but the automotive and mobility industries are among the hardest hit due to corona virus. Consumers across the globe continue to face severe health and financial concerns, and mobility remains somewhat restricted in many parts of the world. Many car dealerships closed, and car buying ground to a halt, with sales plummeting 71 percent in China in February 2020, 47 percent in the US in April, and 80 percent in Europe also in April due to corona virus. Likewise, mobility behavior has changed drastically, as many commuters have worked from home and others have stayed away from public transportation due to health concerns due to corona virus.\\n', 'Now as economies gradually reopen, automotive OEMs, car dealers, and government officials need to know how long the recovery will take and what the next normal will look like due to corona virus. When it comes to buying cars, purchase intent is still 18 percent below pre-COVID-19 levels, albeit with significant geographical differences due to corona virus.\\n', \"The US is down 30 percent, , but recovering (+7 percentage points improvement from the survey's first wave) due to corona virus. Europe is down 20 percent, with UK and Italy being more negatively affected than Germany and France, where purchase intent is recovering significantly (+5 percentage points and +7 percentage points improvement compared to previous survey)\\n\", 'China has recovered, with purchase intent back close to pre-COVID-19 levels due to corona virus\\n', 'Japan is down 25 percent, with no improvement since our first survey due to corona virus\\n', 'Higher-income households, electric vehicles, and premium brands seem less affected due to corona virus\\n', 'Average duration of delay is decreasing, with an increased share of consumers delaying up to 3 months. Delays due to health concerns are decreasing; European consumers increasingly delay to wait for subsidies due to corona virus\\n', 'Across markets, 20 to 40 percent of consumers plan to spend less. While consumers report only limited shifts between new and used cars, and between segments or between volume and premium brands, actual sales suggest that the A and B segments and volume/budget brands may be more negatively affected by COVID-19 due to corona virus.\\n', 'The aftermarket appears to be stable, however, as delayed maintenance and repair seem to be offset by additional and catch-up work due to corona virus.\\n', 'In both buying and servicing, there seems to be a next normal when it comes to how consumers want to buy and service their car due to corona virus. Digital becomes more important along the entire purchase funnel with half of consumers being interested in online and contactless sales and service. Consumers anticipate increasing online ordering of parts and DIY work due to corona virus. During the coronavirus crisis, overall mobility has shrunk across all modes, with trips and commuting now slowly picking up again in all regions. But consumers do not expect big changes in their post-crisis behavior as compared to pre-crisis. More than 2.1 million people around the world have become infected with COVID-19, and more than 140,000 people have died from the disease due to corona virus. The United States, now approaching 650,000 infections, is the new epicenter of the outbreak due to corona virus. But as U.S. officials rush to contain the spread of disease, the federal government is also grappling with the dramatic-and unprecedented-toll the epidemic has had on the economy. In four weeks, 22 million Americans have filed for unemployment benefits. Technical glitches have prevented millions of Americans from receiving their stimulus checks from the U.S. Department of the Treasury. And the Small Business Administration, which supports U.S. entrepreneurs with loans and funding, has run out of money for its Paycheck Protection Program. Due to corona virus\\n', 'In fact, there is no country in the world that can be held up as a model for both its economic and public health response to the coronavirus pandemic.\\n', 'For insights on how U.S. and European governments-and particularly Italy, the previous epicenter of the COVID-19 outbreak-have worked to contain the economic fallout from the global health crisis, the Hub turned to Filippo Taddei, a Johns Hopkins associate professor of international economics and a faculty member at SAIS Europe. The conversation has been edited for length and clarity.\\n', '\\n', '\\n', 'economic fallout from the global health crisis, the Hub turned to Filippo Taddei, a Johns Hopkins associate professor of international economics and a faculty member at SAIS Europe. The conversation has been edited for length and clarity.\\n', \"Over the past several weeks we've seen central banks around the world, particularly the European Central Bank and the Federal Reserve, move with extraordinary speed to shore up financial markets, but these efforts have not calmed volatility. Is there anything else for central bankers to do, or is this an economic crisis that can only be solved through public health measures?\\n\", \"It is true that the size of the intervention is impressive. The size of the Federal Reserve's intervention still remains higher than the ECB, and its promptness to act in the market has been much greater compared to the ECB. Perhaps this is not surprising since the ECB is a combination of the different central banks from EU member countries.\\n\", 'The real difference between the Federal Reserve and the ECB is how timely they have been in their responses. The U.S. started very strongly with a \"preemptive strike\"-style intervention, announcing a rate cut outside of the usual standard monthly meeting. Conversely, the president of the ECB held the usual press conference following the monthly meeting of the bank board, but her language wasn\\'t clear on how much the ECB would act in order to combat the global shock from the pandemic.\\n', 'For central bankers, words often matter more than the actual money, so the wording of statements is crucial, especially at times like these. If we look at the uncertain start by the ECB and quick action by the Federal Reserve, in both cases the real difference is not about the money that central banks can put down, but rather how credible they can be to serve as an anchor against uncertainty.\\n', \"This is a concern for everyone right now-we have a great degree of uncertainty in how long this pandemic will last, and that's fundamental, unfortunately. What we don't want is to add an additional layer of uncertainty about policy. The additional uncertainty is whether our institutions, like the ECB and other central banks, are willing to support the financial sector to make sure that credit keeps on flowing to the real economy, no matter what. This is not as obvious as it might sound: banks hold a large amount of government debt in their balance sheet and, whenever government bonds come under pressure, the increase in their yields threatens the stability of the banking system. When the ECB president asserted that the central bank's job is not to ensure that Euro Area countries' debt trades at low rates, she said something true but self-defeating. During such an unprecedented situation, the last thing a central banker should suggest is that an essential part of private banks' assets could suffer, hindering their ability to operate and extend credit. Facing an unconventional shock, poor messaging and language is a huge drawback-the central banks need to be clearer so that their language matches the extraordinary moment that we're facing.\\n\", 'The current economic crisis calls to mind the Great Recession of 2008 in terms of widespread damage, and some draw comparisons to the Great Depression of the 1930s. Do you feel these are accurate comparisons? Are there other precedents for what we\\'re experiencing, or is it a singular \"black swan\" event?\\n', \"I don't think these are the right comparisons because both crises-the Great Recession and the Great Depression-were essentially demand shocks. What you do with a demand shock is standard macroeconomic policy, and even allowing certain mistakes, we saw in the response to the Great Recession how fiscal and monetary policies worked to alleviate a demand shock.\\n\", \"This is something else. This is supply shock. Here, everything was functioning as normal, but as COVID-19 intensified, bringing thousands and then tens of thousands into the health system, we have decided to shut down the economy. This was because governments discouraged and then prohibited people going to work. If you think about it, supply is the measure of what we collectively produce, but the virus caused a sudden contraction of the labor supply. This has then caused a loss of confidence that resulted in a demand shock, too, but it's a spillover, an indirect effect due to a fundamental contraction in our ability to produce goods and services.\\n\", \"When you face a supply shock, policies like those used during the Great Recession work, but only in containing the secondary shock to people's confidence, the demand shock. It's important to respond on the fiscal and monetary fronts. What's really key is that we don't add additional shocks on top of the initial crisis that is having such a severe effect on our ability to work and produce.\\n\", 'If you want to compare the current crisis to something that happened in the past, a better comparison is the oil shock and energy crisis during the 1970s and early 1980s. The sharp increase in the price of oil made the production and transportation of goods a lot more expensive, hindering productive capacity as is going on right now.\\n', 'In the United States, relief efforts were initially stymied by a lack of consensus on how to allocate resources between working people and industry. How have EU countries navigated this tension, and are there lessons for U.S. lawmakers on crafting an effective stimulus response?\\n', \"When you compare the policy situations in the EU and U.S., keep in mind that the EU is much more gradual in its adjustment. The U.S. is a country of choice and action, where things that seem unmovable before a crisis are suddenly thrown into flux-like the agreement on a $2 trillion stimulus bill. The EU is much more gradual in its approach. While the economic shock is common to all nations, it is not undertaken uniformly. So, what we've seen in Europe is an increasingly stricter response on the health front and an increasingly stronger economic support across the continent, but always undertaken in a gradual fashion.\\n\", \"Europe, and Italy in particular, can serve as a point of observation: if you are too gradual in your response, you run the risk of COVID's course being worse than it might have otherwise have been. Really, Italy's response made sense in the face of an unknown scenario, but perhaps we could have learned a little bit better from the events and responses in Asia. The clear message from our experience is that you need to intervene as swiftly and uniformly as possible. In light of the experience worldwide, one major concern for the U.S. is that different states are acting in different ways in trying to contain the virus.\\n\", 'What are the primary risks for Italy, other EU countries, and the U.S. as the economic crisis precipitated by COVID-19 continues?\\n', \"Global productive capacity has shrunk severely and abruptly as a consequence of lockdown and some needed equipment, like ventilators, is in short supply. In normal times, the economy would quickly adjust by reallocating its workforce through new investments. This is simply impossible when people can't effectively work due to the outbreak.\\n\", 'As overall production of goods and services is reduced, government action ensuring capacity to contain the epidemic as quickly as possible is justified if we want to bring people back to work. This type of policy action makes sense, and the crucial matter is to identify what is the most effective level of authority needed to aggressively address the outbreak. In any case, whether in the U.S. or in Europe, trying to convert production into what is immediately needed to end the outbreak is appropriate.\\n', 'Italy has been encouraging this industrial conversion extensively as well, and so have other countries in Europe. There are different cases of companies that have started producing respirators, masks and protective garments, and other helpful medical supplies.\\n', 'If we want to think of the long-term consequence of the COVID crisis, we should focus on public debt. The Great Recession left us with a legacy in the U.S. and EU of greatly expanded government debt. We think of the Great Recession as a temporary shock that we recovered from but now, as we look at the current crisis, we will be increasing government debt greatly compared to GDP. This is a legacy that will remain for a long time and will pose very pressing policy questions.\\n', 'As we think about the future of advanced economies, in the U.S. and Europe, we have to ask ourselves how we will be dealing with a level of government debt that will exceed, as a share of GDP, the amount we had at the end of WWII. Our management of this new massive debt through the policy response in the aftermath of the crisis will shape our society determining the economic balance between generations, the actual opportunities for future generations, and the technological disruption and transformation that was already in place before this outbreak.\\n', 'COVID-19 has had an unprecedented impact on labor, with the U.S. Treasury Secretary estimating that unemployment could reach 20% in the U.S. What are the long-term impacts, both in Europe and in the United States, of such severe unemployment?\\n', \"We have to be careful not to pay too much attention to the unemployment rate alone as the crisis is also generating substantial underemployment: a large share of the workforce is not able to work as much as they could or wish. In Italy, to give you a sense of the labor situation, only somewhere between 40-50% of the labor force is able to work as efficiently as before. That means that between 50-60% of our workers are either working remotely or not working at all. It's an unprecedented change in peace time, affecting everyone, not just the Italian economy.\\n\", \"There's a large body of literature on the long-term consequences of unemployment, even when due to a short-term shock. When people lose their jobs, the long-lasting effects are not just on their income. Unemployment has a negative effect on workers' skills and education, even on their health-people who are unemployed become sicker. Your human capital, the skills of your country's workforce, decay over time because of the loss of jobs. To mitigate this, the Italian government is doing all it can to keep people as attached to their jobs as possible by preventing companies from enacting layoffs. In order to achieve this objective, short time compensation schemes-usually available only for large industrial firms-have been expanded to almost every sector and firm size. Through these schemes, the government pays reduced salaries, which allows employers to keep their employees without going bankrupt.\\n\", \"In the U.S. these schemes exist in more than 20 states but the country is less equipped in this dimension. U.S. workers experience a quicker turnover: they are laid off more often but then re-hired more quickly compared to the EU. The current scenario is different, though, from the usual business cycle because the current shock could discontinue many of these businesses altogether. What governments need to do at the moment is try to prevent the destruction of capital and desertification of existing businesses. Preventing employers from laying off people is likely to be in their and the economy's best interest, even if they work very little, since this can help to better protect essential human capital. At the moment, the size of resources behind the relief package put in place by the U.S. government has surpassed the combined set of responses taken across Europe.\\n\", 'In the United States, public health officials have looked to Italy to anticipate future scenarios. Do you think this is an apt comparison? What lessons can leaders in the United States and other nations learn from the strategies taken by the Italian government?\\n', \"Yes, it is a possibility, but there are a couple of lessons that Italy's experience can provide in order to prevent or mitigate the outbreak we experienced.\\n\", \"The first is relatively easy: you have to test widely without limiting your attention only to the people showing symptoms. When you test people, keep them separated applying as much social distancing as possible. The U.S., where health care triage is much quicker, plays at an advantage here. These protocols might be more effective right now in containing the spread of the virus. A concern that we have seen in Europe is that if you don't implement a response nationwide, containing the virus will be much harder. The response might not need to be exactly the same everywhere in the country, but you must require coordination and quick scalability. The U.S. must avoid the same mistake we had in Italy and the rest of Europe: if you don't provide a coordinated response to containment, including possible restrictions to the movement and actions of people, the outbreak will only get worse. Make no mistake: this is costly economically, because production contracts sharply across the board, but if you can contain the outbreak in a shorter period of time, you will most likely end up congesting hospital capacity, increase the death toll and, eventually, extend the length of the economic shock.\\n\", 'We are far more connected with one another than we previously thought-not just because our jobs are connected with one another, not just because the value chains are spread throughout our countries, but because our lives are built in connection with one another. COVID-19 is dangerous because it exploits how close we have all become.\\n', '\\n', '\\n', 'Oil has had a turbulent year. Crude prices went negative for the first time in history, followed by one of the biggest rallies the industry has ever seen. And now, just when the market is starting to seem somewhat stable, COVID-19 strikes again.\\n', 'A resurgence of COVID cases in the United States and a gloomy economic forecast from Federal Reserve Chair Jerome Powell has investors scrambling, with oil prices on track to hit their biggest daily decline since April 27th. Once again, oversupply and lack of demand have taken center stage.\\n', 'Yesterday, the EIA reported that U.S. oil inventories rose by 5.7 million barrels, defying predictions of a 1.45 million barrel build. Adding even more pressure to oil prices, the U.S. Federal Reserve noted that unemployment rates were set to settle near 9.3% by the end of the year, adding that it could take years to return to pre-pandemic employment levels.\\n', \"COVID-19 has been the main culprit in the market collapse. While many states have already reopened with some strict guidelines, things don't seem to be going as planned.\\n\", 'It took the United States nearly three months to hit the 1 million confirmed cases mark, yet it only to six weeks to double it. On Wednesday, the U.S. crossed the 2 million mark, with many states reporting significant spikes following attempts to ease lockdown restrictions.\\n', 'Related: Bulls Beware: A Dark Cloud Is Forming Over Oil Markets\\n', 'As of this morning, the U.S. has reported over 113,000 COVID-related deaths, and healthcare experts across the globe are warning that the pandemic is nowhere near over, encouraging individuals to maintain social-distancing practices and to wear face masks in public.\\n', 'Though COVID-19 has taken a clear toll on global economies, some suggest the oil market, in particular, simply rose too quickly.\\n', 'Jeffrey Halley, senior market analyst, Asia-Pacific at OANDA said \"The fall in oil prices is just as much about timing as COVID-19 cases though, coming as both equities and oil were looking overbought on any measure of short-term indicator,\" adding, \"Some sort of correction had been overdue after the massive increase in speculative long positioning, and oil\\'s breath-taking rally over the past month.\"\\n', 'he Great Lockdown continues to turn markets on their head.\\n', 'Last week, we dug into the unprecedented number of initial jobless claims coming out of the United States, which topped 22 million in a period of four weeks.\\n', \"It's just days later, and we already have our next market abnormality: this time, traders were baffled by West Texas Intermediate (WTI) crude - the U.S. benchmark oil price - which somehow flipped negative for the first time in history.\\n\", 'How is that possible? And how does it tie into the COVID-19 oil price crash in general?\\n', 'Setting the Geopolitical Stage\\n', 'Oil is a geopolitical game, and big price swings always come with a geopolitical undercurrent.\\n', 'This particular story picked up steam in February as OPEC+ producers tried to negotiate a production cut, amid concerns that COVID-19 could impact demand. Russia walked out on these meetings, and Saudi Arabia responded by undercutting oil prices by $6-8 per barrel.\\n', 'The world went into lockdown, energy demand dissipated, and oil producers continued to pump at will. Then on April 9th, nearly a full month after COVID-19 was declared a pandemic, Russia and Saudi Arabia finally settled their differences.\\n', 'However, this truce came too late - prices had already fell about 60% from February highs.\\n', 'How Prices Went Subzero\\n', 'Up until recently, this was a fairly run-of-the-mill oil price crash - but then prices suddenly sunk below zero, with May futures for WTI oil closing at -$37.63 on April 20th.\\n', 'For the first time in history, producers were willing to pay traders to take oil off their hands. This oddity is partially a function of the particularities of futures contracts:\\n', 'Buyers Wanted (At Any Cost!)\\n', 'Futures contracts normally rollover to the next month without much happening, but in this case traders saw the May contract as a \"hot potato\". No one wanted to be stuck taking delivery of oil when the world is awash in it and the country is in lockdown.\\n', 'A Time and a Place\\n', \"Oil futures contracts specify a time and place for delivery. For WTI oil, that specific place is Cushing, Oklahoma. With most storage capacity booked already, taking physical delivery wasn't even an option for many players.\\n\", 'In other words, sellers outnumbered buyers by a crazy margin - and because oil is a physical commodity, someone has to ultimately take the contract.\\n', 'At time of publishing, the May contract and spot prices have \"rebounded\" to about $10. The June contract is slightly higher, at $13.\\n', '\"Never before has the oil industry come this close to testing its logistics capacity to the limit.\"\\n', '- International Energy Agency (IEA), Oil Market Report for April\\n', 'Overcoming the Supply Glut\\n', 'What do you do when oil is practically free?\\n', 'You store as much of it as you can, and hope that at some point you can sell it for more.\\n', \"Unfortunately, everyone has the exact same idea, and as a result there is a historic glut that is filling up the world's storage capacity both on land and at sea:\\n\", \"In March, it was estimated that 76% of the world's available oil storage capacity was already full.\\n\", 'A record-setting 160 million barrels of oil is being stored on tankers at sea, according to Reuters.\\n', \"The cost of renting an oil supertanker has gone through the roof. It's jumped from $20,000 per day to $200,000-$300,000 per day, according to Rystad Energy.\\n\", 'It remains to be seen how fast the transportation industry will recover in a post-COVID-19 world, but for now the outlook for all oil producers is grim. The continued fallout will not only affect industry, but also the countries that rely on oil exports to balance their budgets.\\n', '\\n', '\\n', 'Countries in the Middle East and North Africa (MENA) face a dual shock from the coronavirus pandemic (COVID-19) and a collapse in oil prices. To deal with these two shocks, authorities should sequence and tailor their responses. They should focus first on responding to the health emergency and the associated risk of economic depression. They should postpone fiscal consolidation linked to the persistent drop in oil prices and its spillovers until the recovery from the pandemic is well underway.\\n', 'A dual shock\\n', 'Countries in the Middle East and North Africa (MENA) face both the coronavirus pandemic (COVID-19) and a collapse in oil prices.\\n', 'The novel coronavirus - which Chinese authorities first reported to the World Health Organization (WHO) on December 31, 2019 - has spread globally. The virus has infected more than 300,000 people and caused about 13,000 deaths as of March 22, 2020. More than 90,000 individuals have recovered.\\n', \"The virus has seriously affected Iran and has spread to other MENA countries. As of March 22, 2020,_Iran had reported_more than 20,000 infections and more than 1,500 deaths. The rapid rise in infections there is disrupting the country's production and trade. Other MENA countries have also reported infections and also imposed preventive measures. As of March 22, 2020, Saudi Arabia had reported 511 cases; Qatar, 481; Bahrain, 332; Iraq, 233; Kuwait, 188; and the United Arab Emirates UAE), 153.1 Algeria, Egypt, Lebanon, Morocco, and Tunisia have also reported infections. The ability to contain the virus depends on the strength of the region's public health systems-which, with exceptions such as Yemen and Djibouti, the WHO ranks relatively high among the world's 191 health systems_(Tandon and others, 2000). Still, the need for transparency and freer information flow is pervasive. The region risks dramatic consequences if transparency and information flow issues are not addressed expeditiously during this health crisis.2\\n\", 'The virus will not only claim lives. Its spread will confront MENA countries with both a negative supply shock and a negative demand shock (Baldwin and Weder di Mauro, 2020).\\n', 'The negative supply shock comes first from a reduction in labor - directly because workers get sick with COVID-19, the disease caused by the virus, and indirectly due to travel restrictions, quarantine efforts and workers staying home to take care of sick family members or children. Supply will also be affected by a reduction in materials, capital and intermediate inputs due to disruptions in transport and businesses in MENA countries.\\n', \"The negative demand shock is both global and regional. Economic difficulties around the world and the disruption of global value chains will reduce demand for the region's goods and services, most notably oil and tourism. The reduction in GDP growth in China is expected to have a limited effect on tourism activity in MENA (Chart 1). That said, the spread of the virus to other countries, especially in Europe, and the preventive health measures they have taken is expected to have a much larger impact on MENA. Regional demand will also decline as the abrupt reduction in regional business activity and concerns about infection reduce travel. In addition, uncertainty about the spread of the virus and the level of aggregate demand would hurt the region's investment and consumption. Collapsing oil prices further depress demand in MENA, where oil and gas is the most important sector in many economies. Finally, potential financial market volatility could further disrupt aggregate demand._\\n\", '_\\n', 'The negative supply and demand shocks associated with COVID-19 are expected to be relatively short-lived, but dramatic, and to affect many sectors and countries. Demand and supply will recover once the pandemic subsides - and how quickly that occurs depends on the length and depth of the disruption.\\n', \"In addition to the shock from COVID-19, the breakdown in negotiations between the Organization of the Petroleum Exporting Countries (OPEC) and its allies led to what will likely be a persistent collapse in oil prices. On March 5, 2020, OPEC proposed a 1.5 million barrel per day (mb/d) production cut for the second quarter of 2020, of which 1 mb/d would come from OPEC countries and 0.5 mb/d from non-OPEC but aligned producers, most prominently Russia. The following day, Russia rejected the proposal, prompting Saudi Arabia - the world's largest oil exporter - to boost production to 12.3 mb/d, its full capacity. Saudi Arabia also announced unprecedented discounts of almost 20% in key markets. The result was an immediate drop of more than 30% in prices and continuing declines since. The benchmark West Texas Intermediate (WTI) crude oil price reached a low of $22.39 per barrel in the intraday session on March 20, 2020 - less than half the price at the beginning of the month. The futures curve suggests that the market expects oil prices to recover slowly - not reaching $40 per barrel until the end of 2022 (Chart 2). ntertwined but distinct\\n\", 'The two shocks of COVID-19 and oil price collapse are intertwined, yet distinct. On one hand, the demand component of the oil shock is linked to the sharp reduction in oil consumption stemming from precautionary measures to stop the spread of the virus, including lockdowns, which have brought economies around the world to a standstill. The estimated 10% reduction in oil consumption from 2019, or about 10 mb/d, is the result of reduced air and road travel, according to Rystad Energy, the Norwegian research company. While the depth and duration of the pandemic shock is uncertain, it is expected to be short-lived. Indeed, the severity of the shock has triggered unprecedented domestic measures in advanced and developing countries, and the imperative of global coordination to eradicate the virus will hopefully prevail. The international financial institutions are critical to the efforts of developing countries that have acute balance of payments or fiscal problems, and are now fighting COVID-19. These institutions, which can offer zero- to low-interest financing and long maturities, are best-equipped to help countries in MENA and other developing regions deal with the dual shock. The cost of inaction, both economic and social, would be large. The payoff for action is large. Previous experience in fighting smallpox suggested that the benefit-cost ratio for assisting its eradication of the disease exceeded 400-1 (Barrett, 2007).\\n', 'Once the spread of the virus is stopped, the preventive measures at the root of the economic recession will be rolled back. The speed of that recovery will depend on how swiftly and decisively governments take measures to mitigate the economic and financial dislocations from the health crisis. But the supply component of the oil shock is likely to be persistent and drive oil prices lower for longer. The two shocks differ in their duration but also their likely potential consequences and associated risks of inaction.\\n', 'When assessing the impact of oil prices on the global economy, economists typically distinguish between supply- and demand-driven oil shocks. Demand-driven shocks are related to the evolution of global demand and as such are not expected to have an independent effect on the global economy. But the supply-driven oil shocks would normally be expected to give an independent boost to the global economy. There are several reasons why they might not - in good part because the financial propagation effects of the collapse in oil prices have caused markets for equities, bonds, and non-oil commodities to tumble.\\n', 'For MENA countries specifically, lower prices are generally good for oil-importing countries and bad for oil exporters. A simple way to get a sense of the size of the real income effect of an oil price change is to multiply the difference between production and consumption (net oil export) as a share of GDP by the percentage point change in the oil price. For instance, based on hypothetical assumption that oil prices will stay 48% below the 2019 level, Kuwait - where net oil exports account for 43% of GDP - would experience a decline in real income of about 20% of GDP, while importer Morocco would experience an increase in real income equivalent to 3% of GDP (Chart 3).\\n', '_\\n', '\\n', '\\n', 'But in MENA it is likely that lower oil prices will hurt both importers and exporters - exporters directly and importers indirectly from reduced foreign direct investment, remittances, tourism, and grants from exporters.3 4 Some countries, such as those in the Gulf Cooperation Council, still have buffers and should use them. Other oil-exporting countries, such as Algeria and Iran, are exhausting their buffers and will have to rely on flexible exchange rates to manage the current situation and conduct much needed reforms in private-sector development and broader economic transformation. Among net oil importers - such as Egypt, Jordan and Lebanon - a recession will worsen already high levels of public debt.\\n', 'Policy response_\\n', 'While COVID-19 has caused a severe supply shock that is expected to increase_unemployment and poverty, there is also a sizable feedback loop in terms of demand. Besides the loss of human lives, inaction also risks massive disruptions in supply and demand as well as illiquidity in the financial sector. In other words, the COVID-19 shock could lead to household and corporate bankruptcies, with lasting scars on the economy and society. The disruptions come at a time of discontent in MENA where the streets have been full of protests demanding better governance and an end to corruption.\\n', \"In contrast, the oil price collapse is a commodity terms-of-trade shock that affects the economy through reduced export receipts and revenues in government coffers. The shock is expected to be persistent and lead to widening twin deficits - in a country's current balance and its government budget - as well as increased debt if there is no fiscal consolidation.\\n\", 'To deal with the dual shock, authorities should sequence and tailor their responses to the severity of the shocks. They should focus first on responding to the health emergency and the associated risk of economic depression. Authorities should postpone the fiscal consolidation associated with the persistent drop in oil price and its spillovers until the recovery from the pandemic is well underway. In responding to COVID-19, authorities should boost spending on health - including producing or acquiring test kits, mobilizing and paying health workers, adding health infrastructure, and preparing for vaccination campaigns. The authorities should also use targeted cash transfers to vulnerable households and support the private sector, including small- and medium-sized enterprises in the informal sector.\\n', 'It is paramount to target the large number of workers in the informal sector, who have no safety net. Many of them work hand-to-mouth. Given the large labor share and borrowing constraints in many developing countries across MENA, targeted assistance is vital and should be larger relative to the economy than similar efforts in advanced economies. Successful models of quickly deploying the role of technology, including digital, to fight COVID-19 and target assistance should be analyzed and replicated.5_As mentioned earlier, freeing information flows, increasing transparency and data disclosure to reduce leakages are crucial elements in target cash transfers, which themselves will be essential to ensuring a flattening of the spread of the virus, hastening the economic recovery and limiting the rise in poverty. To limit risk of financial instability, countries in the MENA region should reduce interest rates and inject liquidity in the banking system. Where inflation is low, liquidity injection and targeted cash transfers can be financed by \"helicopter money,\" that is, essentially from money printed by central banks (Gali, 2020).\\n', 'The battle against the spread of the novel coronavirus and its economic and social consequences will be made more difficult by empty government coffers. Many MENA countries are facing large balance-of-payments and fiscal gaps. Many also carry high sovereign-risk premiums. For those countries, additional foreign borrowing on private markets will be difficult. Moreover, countries with fixed exchange rates will find it difficult to use helicopter money because of the tension between money printing and maintenance of the peg. The region will need much international support to help it navigate an extremely rough patch._Oil prices fell on Wednesday as investors worried about fuel demand due to fresh outbreaks of COVID-19, though prices drew some support after US stocks of diesel fuel fell for the first time in weeks and US oil production dropped sharply.However, the virus is spreading in parts of the United States, while scores of flights were cancelled and schools were shut in Beijing to head off a new virus outbreak in the Chinese city.\\n', '\\n', '\\n', '\"We think the oil market is not currently pricing in a significant probability of either second waves of coronavirus cases in key consumers and the associated lockdowns, or anything less than a rapid return to economic business-as-usual,\" Standard Chartered analysts said. As lockdowns forced factories to close and people to stop travelling, our global demand for oil has reduced by 29 million barrels a day.\\n', 'With the benchmark price now lower than the cost of production, transport and storage, how will big oil producers like Russia and Saudi Arabia fare in the coming year?\\n', \"It's hard to believe that the price of any commodity, let alone oil, can dip into negative territory. But that's just what's happened to oil prices.\\n\", 'COVID-19 has prompted lockdowns, shuttered factories and stopped people from travelling. The global economy is contracting.\\n', 'The pandemic has also reduced global demand for oil by about 29 million barrels a day from about 100 million a year ago. OPEC and other producers agreed to cut production by 9.7 million barrels a day, far less than the decrease in demand, leaving a huge surplus of oil on the market and no buyers.\\n', 'Storage capacity on land has filled up quickly. Many oil-importing countries have stored large quantities of oil, taking advantage of cheap prices that may not last. Oil prices will come back up\\n', 'So how have Alberta oil prices and even future prices for West Texas Intermediate (WTI) slipped into negative territory?\\n', \"It starts with the futures' contracts for WTI - oil to be delivered in a few months at today's price. It lost US$6 a barrel on Monday, fetching US$11.66, but ended the day at -US$37 as holders of future contracts tried to dump their contracts before oil is actually delivered with nowhere to store it.\\n\", 'But Alberta oil, primarily derived from oilsands (referred to as Western Select), typically sells at US$10 to US$15 below the price of WTI, because it has to be extracted from deep rocky terrain. That makes it harder to refine, and it also has to be transported thousands of kilometres to American refineries.\\n', 'And so Alberta oil prices have become negative in the sense that the benchmark price is now lower than the cost of production, transport and storage.\\n', 'This state of affairs cannot be expected to last for long. Producers, in the short term, may accept prices below their variable cost as long as they are able to pay some of the costs they will incur even if oil production shuts down.\\n', \"As time passes, more and more rigs will stop operating (technically, a few will be kept operational in order to avoid being compromised) and a new balance between supply and demand will be established at prices that exceed total average cost. But this doesn't bode well for either Alberta or the United States.\\n\", 'Collateral damage\\n', \"Alberta oil is now the collateral damage of the oil war between Russia and Saudi Arabia, with COVID-19 launching an additional attack. Either of these two factors could have disrupted Alberta's oil production. But the Saudi-Russia hostilities combined with the global pandemic have proven to be catastrophic for Canada, and could have a similar outcome for the U.S. energy industry.\\n\", \"Russia and Saudi Arabia depend heavily on their oil revenues to sustain their economies. Of course, Saudi Arabia's economy is less diversified than the Russian economy, but both share a similar distortion, where oil revenues represent a very high share of their GDPs (Saudi Arabia about 50 per cent, Russia 38.9 per cent), budgets (Saudi Arabia 87 per cent and Russia 68 per cent) and exports (Saudi Arabia 90 per cent and Russia 59 per cent. It's difficult to believe that either country can do with such low prices.\\n\", 'Russia needs a price of US$60 a barrel to balance its government budget and even a higher price to balance its current account, meaning exports of goods and services minus imports of goods and services, plus net short-term capital transfers.\\n', 'Saudis also need a much higher oil price\\n', 'Saudi Arabia, which remains the lowest-cost oil producer in the world, can make money when the price per barrel exceeds US$20, and Russia can at a price of US$40.\\n', 'But making a profit when prices are higher than cost is not sufficient. Saudi Arabia needs an US$80-per-barrel price to balance its budget, realize its plans to diversify its economy and sustain a heavily subsidized economy. In the balance is the stability of both the Russian and Saudi Arabian political systems and current regimes.\\n', \"The longer the COVID-19 pandemic lasts, the greater the damage oil producers will endure. It's hard to tell now how high oil prices will rise once the pandemic subsides. They will likely go higher as marginal producers are eliminated, but not for long. Using oil and other fossil fuels is no longer consistent with avoiding the expected disasters of climate change. Oil is increasingly becoming a stranded asset.\\n\", 'The COVID-19 pandemic has had far-reaching consequences beyond the spread of the disease itself and efforts to quarantine it. As the SARS-CoV-2 virus has spread around the globe, concerns have shifted from supply-side manufacturing issues to decreased business in the services sector.[1] The pandemic caused the largest global recession in history, with more than a third of the global population at the time being placed on lockdown.[2] In turn, it caused the largest and worst civil unrest in the United States history since King assassination riots in 1968,[3] which worsening economic impact caused by pandemic since it contains extensive property damage.[4]\\n', 'Supply shortages are expected to affect a number of sectors due to panic buying, increased usage of goods to fight the pandemic, and disruption to factories and logistics in mainland China. There have been instances of price gouging.[5] There have been widespread reports of shortages of pharmaceuticals,[6] with many areas seeing panic buying and consequent shortages of food and other essential grocery items.[7][8][9] The technology industry, in particular, has been warning about delays to shipments of electronic goods.[10][needs update]\\n', \"Global stock markets fell on 24 February 2020 due to a significant rise in the number of COVID-19 cases outside mainland China.[11][12] By 28 February 2020, stock markets worldwide saw their largest single-week declines since the 2008 financial crisis.[13][14][15] Global stock markets crashed in March 2020, with falls of several percent in the world's major indices. As the pandemic spreads, global conferences and events across technology, fashion, and sports are being cancelled or postponed.[16] While the monetary impact on the travel and trade industry is yet to be estimated, it is likely to be in the billions and increasing. The pandemic coincided with the Chunyun, a major travel season associated with the Chinese New Year holiday. A number of events involving large crowds were cancelled by national and regional governments, including annual New Year festivals, with private companies also independently closing their shops and tourist attractions such as Hong Kong Disneyland and Shanghai Disneyland.[18][19] Many Lunar New Year events and tourist attractions were closed to prevent mass gatherings, including the Forbidden City in Beijing and traditional temple fairs.[20] In 24 of China's 31 provinces, municipalities and regions, authorities extended the New Year's holiday to 10 February, instructing most workplaces not to re-open until that date.[21][22] These regions represented 80% of the country's GDP and 90% of exports.[22] Hong Kong raised its infectious disease response level to the highest and declared an emergency, closing schools until March and cancelling its New Year celebrations.[23][24]\\n\", 'The demand for personal protection equipment has risen 100-fold, according to WHO director-general Tedros Adhanom. This demand has led to an increase in prices of up to twenty times the normal price and also induced delays on the supply of medical items for four to six months. he pandemic may have improved scientific communication or established new forms of it. For instance, a lot of data is being released on preprint servers and is getting dissected on social Internet platforms and sometimes in the media before entering formal peer review. Scientists are reviewing, editing, analysing and publishing manuscripts and data at record speeds and in large numbers. This intense communication may have allowed an unusual level of collaboration and efficiency among scientists.[4] Francis Collins notes that while he hasn\\'t seen research move faster, the pace of research \"can still feel slow\" during a pandemic. The typical model for research has been considered too slow for the \"urgency of the coronavirus threat\".[5] A number of factors shape how much and which scientific knowledge can be established timely.[citation needed]\\n', 'World Health Organization[edit]\\n', 'On 4_May 2020, the World Health Organization (WHO) organised a telethon to raise US$8 billion from forty countries to support rapid development of vaccines to prevent COVID-19 infections,[6] also announcing deployment of an international \"Solidarity trial\" for simultaneous evaluation of several vaccine candidates reaching Phase II-III clinical trials.[7] The \"Solidarity trial for treatments\" is a multinational Phase III-IV clinical trial organised by the WHO and partners to compare four untested treatments for hospitalised people with severe COVID-19 illness.[8][9] The trial was announced 18 March 2020,[8] and as of 21 April, over 100 countries were participating.[10] The WHO is also coordinating a multiple-site, international randomised controlled trial-the \"Solidarity trial for vaccines\"[7][11]-to enable simultaneous evaluation of the benefits and risks of different vaccine candidates under clinical trials in countries where there are high rates of COVID-19 disease, ensuring fast interpretation and sharing of results around the world.[7] The WHO vaccine coalition will prioritize which vaccines should go into Phase II and III clinical trials, and determine harmonised Phase III protocols for all vaccines achieving the pivotal trial stage.[7]\\n', 'The Coalition for Epidemic Preparedness Innovations (CEPI)-which is organising a US$2 billion worldwide fund for rapid investment and development of vaccine candidates[12]-indicated in April that a vaccine may be available under emergency use protocols in less than 12 months or by early 2021.[13]National and intergovernmental laboratories[edit]\\n', 'United States Department of Energy federal scientific laboratories such as the Oak Ridge National Laboratory have closed all its doors to all visitors and many employees, and non-essential staff and scientists are required to work from home if possible. Contractors also are strongly advised to isolate their facilities and staff unless necessary. The overall operation of the ORNL remains somewhat unaffected.[14]\\n', 'The Lawrence Livermore National Laboratory has been tasked by the White House Coronavirus Task Force to utilize most of its supercomputing capability for further research of the virus stream, possible mutations and other factors; while temporary reducing other projects or delaying them indefinitely.[15]\\n', \"The European Molecular Biology Laboratory has closed all six sites across Europe (Barcelona, Grenoble, Hamburg, Heidelberg, Hinxton and Rome). All of EMBL's host governments have introduced strict controls in response to the coronavirus. EMBL staff have been instructed to follow local government advice. A small number of staff have been authorised to attend the sites to provide an essential service such as maintenance of animal facilities or data services. All other staff have been instructed to stay at home. EMBL has also cancelled all visits to sites by non-staff groups. This includes physical participation in the Courses and Conferences programme at Heidelberg, the EMBL-EBI Training courses, and all other seminars, courses and public visits at all sites. Meanwhile, the European Bioinformatics Institute is creating a European COVID-19 Data Platform for data/information exchange. The goal is to collect and share rapidly available research data to enable synergies, cross-fertilisation and use of diverse data sets with different degrees of aggregation, validation and/or completeness. The platform is envisaged to consist of two connected components, the SARS-CoV-2 Data Hubs organising the flow of SARS-CoV-2 outbreak sequence data and providing comprehensive open data sharing for the European and global research communities, and one broader COVID-19 Portal.[16][17][18]\\n\", 'The World Meteorological Organization expressed concern about the observation system. Observations from the Aircraft Meteorological Data Relay programme, which uses in-flight measurements from the fleets of 43 airlines, were reduced by 50% to 80% depending on region. Data from other automated systems was largely unaffected, though the WMO expressed fears that repairs and maintenance may be affected. Manual observations, mostly from developing countries, also saw a significant decrease.[19]\\n', 'Open science[edit]\\n', 'The need for accelerating open scientific research made several civil society organisations to create an Open COVID Pledge[20][21] asking to different industries to release their intellectual property rights during the pandemic to help find a cure for the disease. Several tech giants joined the pledge.[22] The pledge includes the release of an Open COVID license.[23] Organisations that have been longtime advocates for Open Access, such as Creative Commons, implemented a myriad of calls and actions to promote open access in science as a key element to combat the disease.[24][25] These include a public call for more open access policies,[26] and a call to scientists to adopt zero embargo periods for their publications, implementing a CC BY to their articles, and a CC0 waiver for the research data.[27] Other organisations questioned the current scientific culture, making a call for more open, public science.[28]\\n', 'Computing research and citizen science[edit]\\n', 'In March 2020, the United States Department of Energy, National Science Foundation, NASA, industry, and nine universities pooled resources to access supercomputers from IBM, combined with cloud computing resources from Hewlett Packard Enterprise, Amazon, Microsoft, and Google, for drug discovery.[29][30] The COVID-19 High Performance Computing Consortium is also being used to forecast disease spread, model possible vaccines, and screen thousands of chemical compounds to design a COVID-19 vaccine or therapy.[29][30]\\n', 'The C3.ai Digital Transformation Institute, an additional consortium of Microsoft, six universities (including the Massachusetts Institute of Technology, a member of the first consortium), and the National Center for Supercomputer Applications in Illinois, working under the auspices of C3.ai, a company founded by Thomas Siebel, are pooling supercomputer resources toward drug discovery, medical protocol development and public health strategy improvement, as well as awarding large grants to researchers who propose to use AI to carry out similar tasks by May.[31][32]\\n', 'In March 2020, the distributed computing project Folding@home launched a program to assist medical researchers around the world. The initial wave of projects are meant to simulate potentially protein targets from SARS-CoV-2 virus, and the related SARS-CoV virus, which has been studied previously.[33][34][35]\\n', 'In May 2020, the OpenPandemics-COVID-19 partnership between Scripps Research and IBM\\'s World Community Grid was launched. The partnership is a distributed computing project that \"will automatically run a simulated experiment in the background [of connected home PCs] which will help predict the effectiveness of a particular chemical compound as a possible treatment for COVID-19\".[36]\\n', 'Resources for computer science and scientific crowdsourcing projects concerning COVID-19 can be found on the internet or as apps.[37][38][39] Examples of such projects are listed below:\\n', 'The Eterna OpenVaccine project enables video game players to \"design an mRNA encoding a potential vaccine against the novel coronavirus.\"[40]\\n', 'The EU-Citizen.Science project has \"a selection of resources related to the current COVID19 pandemic. It contains links to citizen science and crowdsourcing projects\"[41]\\n', 'The COVID-19 Citizen Science project is \"a new initiative by University of California, San Francisco physician-scientists\" that \"will allow anyone in the world age 18 or over to become a citizen scientist advancing understanding of the disease.\"[42]\\n', 'The CoronaReport digital journalism project is \"a citizen science project which democratizes the reporting on the Coronavirus, and makes these reports accessible to other citizens.\"[43][44]\\n', 'The COVID Symptom Tracker is a crowdsourced study of the symptoms of the virus. It has had two million downloads by April 2020.[45][46]\\n', 'The Covid Near You epidemiology tool \"uses crowdsourced data to visualise maps to help citizens and public health agencies identify current and potential hotspots for the recent pandemic coronavirus, COVID-19.\"[47]\\n', '\\n', '\\n', \"The James Webb Space Telescope's launch has been delayed.\\n\", '\\n', '\\n', '\\n', '\\n', 'Components of the Space Launch System\\n', 'NASA announced the temporary closure of all its field centre visitor complexes until further notice, as well as requiring all non-critical personnel to work from home if possible. Production and manufacture of the Space Launch System at the Michoud Assembly Facility was stopped,[48][49] and further delays to the James Webb Space Telescope are expected,[50] though as of 3 June work has resumed.[51]\\n', 'The majority of personnel at the Johnson Space Center transitioned to teleworking, and International Space Station mission critical personnel were instructed to reside in the mission control room until further notice. Station operations are relatively unaffected, but new expedition astronauts face longer and stricter quarantines before flight.[52]\\n', \"NASA's emergency response framework has varied depending on local virus cases around its agency field centres. As of 24 March 2020, the following space centres had been escalated to stage 4:[citation needed]\\n\", 'Glenn Research Center in Ohio\\n', 'Plum Brook Station in Ohio\\n', 'Armstrong Flight Research Center in California\\n', 'Wallops Flight Facility in Virginia\\n', 'Goddard Institute for Space Studies in New York\\n', 'Goddard Space Flight Center in Maryland, who also reported its first case of an employee testing positive for COVID-19.\\n', 'Two facilities were held at stage 4 after reporting new coronavirus cases: the Michoud Assembly Facility reporting its first employee testing positive for COVID-19, and the Stennis Space Center recording a second case of a member of the NASA community with the virus. The Kennedy Space Center was held at stage 3, after one member of the workforce tested positive. Due to mandatory telework policy already in effect, the individual had not been on site for over a week prior to symptoms.[citation needed] On May 18, the Michoud facility began to resume SLS work operations, but so far remains in a state of level 3.[53]\\n', 'At stage 4, mandatory telework is in effect for all personnel, with the exception of limited personnel required for mission-essential work and to care-take and maintain the safety and security of the facility.[54]\\n', 'ESA\\n', '_']\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At-tEFKN-xtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(news_feed[:10])\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNbaA6Zqn15O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "f = open('stock_twitter_news.txt', 'w')\n",
        "f.write('.'.join(news_feed))\n",
        "f.write(x)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti_7RYkK-4KS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "\n",
        "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
        "\n",
        "\n",
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Load Dataset from File\n",
        "    \"\"\"\n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file, \"r\") as f:\n",
        "        data = f.read()\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def preprocess_and_save_data(dataset_path, token_lookup, create_lookup_tables):\n",
        "    \"\"\"\n",
        "    Preprocess Text Data\n",
        "    \"\"\"\n",
        "    text = load_data(dataset_path)\n",
        "    \n",
        "    # Ignore notice, since we don't use it for analysing the data\n",
        "    text = text[81:]\n",
        "\n",
        "    token_dict = token_lookup()\n",
        "    for key, token in token_dict.items():\n",
        "        text = text.replace(key, ' {} '.format(token))\n",
        "\n",
        "    text = text.lower()\n",
        "    text = text.split()\n",
        "\n",
        "    vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
        "    int_text = [vocab_to_int[word] for word in text]\n",
        "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))\n",
        "\n",
        "\n",
        "def load_preprocess():\n",
        "    \"\"\"\n",
        "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
        "    \"\"\"\n",
        "    return pickle.load(open('preprocess.p', mode='rb'))\n",
        "\n",
        "\n",
        "def save_model(filename, decoder):\n",
        "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
        "    torch.save(decoder, save_filename)\n",
        "\n",
        "\n",
        "def load_model(filename):\n",
        "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
        "    return torch.load(save_filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U4CntYOzQC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "# load in data\n",
        "\n",
        "data_dir = 'stock_twitter_news.txt'\n",
        "text = load_data(data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLpK9exu1Enq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "1002c426-4826-487d-c82c-89514f841352"
      },
      "source": [
        "view_line_range = (11720, 11730)\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "print('Dataset Stats')\n",
        "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
        "\n",
        "lines = text.split('\\n')\n",
        "print('Number of lines: {}'.format(len(lines)))\n",
        "word_count_line = [len(line.split()) for line in lines]\n",
        "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
        "\n",
        "print()\n",
        "print('The lines {} to {}:'.format(*view_line_range))\n",
        "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Stats\n",
            "Roughly the number of unique words: 159347\n",
            "Number of lines: 68464\n",
            "Average number of words in each line: 29.86134318766067\n",
            "\n",
            "The lines 11720 to 11730:\n",
            "- Consolidated Net Revenue Increased 11.7% to $3.44 Billion\n",
            "- Net Income Increased 124.1% to $1.36 Billion\n",
            "- GAAP Earnings per Diluted Share Increased 139.1% to $1.53; Adjusted Earnings per Diluted Share Increased 41.9% to $0.88\n",
            "- Due to U.S. Tax Reform, Net Income Includes a Nonrecurring Non-Cash Income Tax Benefit of $526 Million and Adjusted Earnings per Diluted Share Excludes the $0.66 Impact per Diluted Share\n",
            "- Consolidated Hold-Normalized Adjusted Property EBITDA Increased 18.9% to $1.29 Billion, While Adjusted Property EBITDA Increased 19.7% to $1.34 Billion\n",
            "- In Macao, Adjusted Property EBITDA Increased 19.8% to $731 Million, While Hold-Normalized Adjusted Property EBITDA Increased 30.0% to $758 Million\n",
            "- At Marina Bay Sands in Singapore, Adjusted Property EBITDA Increased 24.6% to $456 Million, While Hold-Normalized Adjusted Property EBITDA Increased 6.0% to $388 Million\n",
            "- At Our Las Vegas Operating Properties, Adjusted Property EBITDA Increased 2.7% to $114 Million\n",
            "- The Company Paid Quarterly Dividends of $0.73 per Share\n",
            "- The Company Repurchased $75 Million of Common Stock\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trZWjzph1Mn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def create_lookup_tables(text):\n",
        "    \"\"\"\n",
        "    Create lookup tables for vocabulary\n",
        "    :param text: The text of tv scripts split into words\n",
        "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    word_count = Counter(text)\n",
        "    sorted_vocab = sorted(word_count, key = word_count.get, reverse=True)\n",
        "    int_to_vocab = {ii:word for ii, word in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {word:ii for ii, word in int_to_vocab.items()}\n",
        "    \n",
        "    # return tuple\n",
        "    return (vocab_to_int, int_to_vocab)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvE9O0F31Sa5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def token_lookup():\n",
        "    \"\"\"\n",
        "    Generate a dict to turn punctuation into a token.\n",
        "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    token = dict()\n",
        "    token['.'] = '<PERIOD>'\n",
        "    token[','] = '<COMMA>'\n",
        "    token['\"'] = 'QUOTATION_MARK'\n",
        "    token[';'] = 'SEMICOLON'\n",
        "    token['!'] = 'EXCLAIMATION_MARK'\n",
        "    token['?'] = 'QUESTION_MARK'\n",
        "    token['('] = 'LEFT_PAREN'\n",
        "    token[')'] = 'RIGHT_PAREN'\n",
        "    token['-'] = 'QUESTION_MARK'\n",
        "    token['\\n'] = 'NEW_LINE'\n",
        "    return token\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_sNwT901WUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "# pre-process training data\n",
        "preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lCVmtHB1Y8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "import helper\n",
        "\n",
        "int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T1ovulP1bQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "# Check for a GPU\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if not train_on_gpu:\n",
        "    print('No GPU found. Please use a GPU to train your neural network.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LwWUJmz1gya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def batch_data(words, sequence_length, batch_size):\n",
        "    \"\"\"\n",
        "    Batch the neural network data using DataLoader\n",
        "    :param words: The word ids of the TV scripts\n",
        "    :param sequence_length: The sequence length of each batch\n",
        "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
        "    :return: DataLoader with batched data\n",
        "    \"\"\"\n",
        "    # TODO: Implement function\n",
        "    n_batches = len(words)//batch_size\n",
        "    x, y = [], []\n",
        "    words = words[:n_batches*batch_size]\n",
        "    \n",
        "    for ii in range(0, len(words)-sequence_length):\n",
        "        i_end = ii+sequence_length        \n",
        "        batch_x = words[ii:ii+sequence_length]\n",
        "        x.append(batch_x)\n",
        "        batch_y = words[i_end]\n",
        "        y.append(batch_y)\n",
        "    \n",
        "    data = TensorDataset(torch.from_numpy(np.asarray(x)), torch.from_numpy(np.asarray(y)))\n",
        "    data_loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n",
        "        \n",
        "    \n",
        "    # return a dataloader\n",
        "    return data_loader\n",
        "\n",
        "# there is no test for this function, but you are encouraged to create\n",
        "# print statements and tests of your own\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-nSbEYC1yBt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "22217c6f-e188-4883-96d2-69909762a5fa"
      },
      "source": [
        "# test dataloader\n",
        "\n",
        "test_text = range(50)\n",
        "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
        "\n",
        "data_iter = iter(t_loader)\n",
        "sample_x, sample_y = data_iter.next()\n",
        "\n",
        "print(sample_x.shape)\n",
        "print(sample_x)\n",
        "print()\n",
        "print(sample_y.shape)\n",
        "print(sample_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 5])\n",
            "tensor([[14, 15, 16, 17, 18],\n",
            "        [15, 16, 17, 18, 19],\n",
            "        [20, 21, 22, 23, 24],\n",
            "        [13, 14, 15, 16, 17],\n",
            "        [ 3,  4,  5,  6,  7],\n",
            "        [11, 12, 13, 14, 15],\n",
            "        [ 8,  9, 10, 11, 12],\n",
            "        [19, 20, 21, 22, 23],\n",
            "        [43, 44, 45, 46, 47],\n",
            "        [38, 39, 40, 41, 42]])\n",
            "\n",
            "torch.Size([10])\n",
            "tensor([19, 20, 25, 18,  8, 16, 13, 24, 48, 43])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsGAqMS52E3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the PyTorch RNN Module\n",
        "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
        "        :param output_size: The number of output dimensions of the neural network\n",
        "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
        "        :param hidden_dim: The size of the hidden layer outputs\n",
        "        :param dropout: dropout to add in between LSTM/GRU layers\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "        # TODO: Implement function\n",
        "        \n",
        "        # define embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # define lstm layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "        \n",
        "        \n",
        "        # set class variables\n",
        "        self.vocab_size = vocab_size\n",
        "        self.output_size = output_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        # define model layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation of the neural network\n",
        "        :param nn_input: The input to the neural network\n",
        "        :param hidden: The hidden state        \n",
        "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
        "        \"\"\"\n",
        "        # TODO: Implement function   \n",
        "        batch_size = x.size(0)\n",
        "        x=x.long()\n",
        "        \n",
        "        # embedding and lstm_out \n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        \n",
        "        # stack up lstm layers\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout, fc layer and final sigmoid layer\n",
        "        out = self.fc(lstm_out)\n",
        "        \n",
        "        # reshaping out layer to batch_size * seq_length * output_size\n",
        "        out = out.view(batch_size, -1, self.output_size)\n",
        "        \n",
        "        # return last batch\n",
        "        out = out[:, -1]\n",
        "\n",
        "        # return one batch of output word scores and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        '''\n",
        "        Initialize the hidden state of an LSTM/GRU\n",
        "        :param batch_size: The batch_size of the hidden state\n",
        "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
        "        '''\n",
        "        # create 2 new zero tensors of size n_layers * batch_size * hidden_dim\n",
        "        weights = next(self.parameters()).data\n",
        "        if(train_on_gpu):\n",
        "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(), \n",
        "                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                     weights.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        # initialize hidden state with zero weights, and move to GPU if available\n",
        "        \n",
        "        return hidden\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UURpsD-g2ciF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
        "    \"\"\"\n",
        "    Forward and backward propagation on the neural network\n",
        "    :param decoder: The PyTorch Module that holds the neural network\n",
        "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
        "    :param criterion: The PyTorch loss function\n",
        "    :param inp: A batch of input to the neural network\n",
        "    :param target: The target output for the batch of input\n",
        "    :return: The loss and the latest hidden state Tensor\n",
        "    \"\"\"\n",
        "    \n",
        "    # TODO: Implement Function\n",
        "    \n",
        "    # move data to GPU, if available\n",
        "    if(train_on_gpu):\n",
        "        rnn.cuda()\n",
        "    \n",
        "    # creating variables for hidden state to prevent back-propagation\n",
        "    # of historical states \n",
        "    h = tuple([each.data for each in hidden])\n",
        "    \n",
        "    rnn.zero_grad()\n",
        "    # move inputs, targets to GPU \n",
        "    if(train_on_gpu):\n",
        "        inputs, targets = inp.cuda(), target.cuda()\n",
        "    \n",
        "    output, h = rnn(inputs, h)\n",
        "    \n",
        "    loss = criterion(output, targets)\n",
        "    \n",
        "    # perform backpropagation and optimization\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
        "    optimizer.step()\n",
        "\n",
        "    # return the loss over a batch and the hidden state produced by our model\n",
        "    return loss.item(), h\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5knZB_P2rqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "\n",
        "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
        "    batch_losses = []\n",
        "    \n",
        "    rnn.train()\n",
        "\n",
        "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
        "    for epoch_i in range(1, n_epochs + 1):\n",
        "        \n",
        "        # initialize hidden state\n",
        "        hidden = rnn.init_hidden(batch_size)\n",
        "        \n",
        "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
        "            \n",
        "            # make sure you iterate over completely full batches, only\n",
        "            n_batches = len(train_loader.dataset)//batch_size\n",
        "            if(batch_i > n_batches):\n",
        "                break\n",
        "            \n",
        "            # forward, back prop\n",
        "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
        "            # record loss\n",
        "            batch_losses.append(loss)\n",
        "\n",
        "            # printing loss stats\n",
        "            if batch_i % show_every_n_batches == 0:\n",
        "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
        "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
        "                batch_losses = []\n",
        "\n",
        "    # returns a trained rnn\n",
        "    return rnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qhdC7oO2wyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data params\n",
        "# Sequence Length\n",
        "sequence_length = 10  # of words in a sequence\n",
        "# Batch Size\n",
        "batch_size = 128\n",
        "\n",
        "# data loader - do not change\n",
        "train_loader = batch_data(int_text, sequence_length, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4aiXhBi20wl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training parameters\n",
        "# Number of Epochs\n",
        "num_epochs = 10\n",
        "# Learning Rate\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Model parameters\n",
        "# Vocab size\n",
        "vocab_size = len(vocab_to_int)\n",
        "# Output size\n",
        "output_size = vocab_size\n",
        "# Embedding Dimension\n",
        "embedding_dim = 200\n",
        "# Hidden Dimension\n",
        "hidden_dim = 250\n",
        "# Number of RNN Layers\n",
        "n_layers = 2\n",
        "\n",
        "# Show stats for every n number of batches\n",
        "show_every_n_batches = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrFjAYqe229y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL\n",
        "\"\"\"\n",
        "\n",
        "# create model and move to gpu if available\n",
        "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
        "if train_on_gpu:\n",
        "    rnn.cuda()\n",
        "\n",
        "# defining loss and optimization functions for training\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# training the model\n",
        "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, 5, show_every_n_batches)\n",
        "\n",
        "# saving the trained model\n",
        "save_model('./save/trained_rnn', trained_rnn)\n",
        "print('Model Trained and Saved')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMRGOJPW28pQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
        "    \"\"\"\n",
        "    Generate text using the neural network\n",
        "    :param decoder: The PyTorch Module that holds the trained neural network\n",
        "    :param prime_id: The word id to start the first prediction\n",
        "    :param int_to_vocab: Dict of word id keys to word values\n",
        "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
        "    :param pad_value: The value used to pad a sequence\n",
        "    :param predict_len: The length of text to generate\n",
        "    :return: The generated text\n",
        "    \"\"\"\n",
        "    rnn.eval()\n",
        "    \n",
        "    # create a sequence (batch_size=1) with the prime_id\n",
        "    current_seq = np.full((1, sequence_length), pad_value)\n",
        "    current_seq[-1][-1] = prime_id\n",
        "    predicted = [int_to_vocab[prime_id]]\n",
        "    \n",
        "    for _ in range(predict_len):\n",
        "        if train_on_gpu:\n",
        "            current_seq = torch.LongTensor(current_seq).cuda()\n",
        "        else:\n",
        "            current_seq = torch.LongTensor(current_seq)\n",
        "        \n",
        "        # initialize the hidden state\n",
        "        hidden = rnn.init_hidden(current_seq.size(0))\n",
        "        \n",
        "        # get the output of the rnn\n",
        "        output, _ = rnn(current_seq, hidden)\n",
        "        \n",
        "        # get the next word probabilities\n",
        "        p = F.softmax(output, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "         \n",
        "        # use top_k sampling to get the index of the next word\n",
        "        top_k = 5\n",
        "        p, top_i = p.topk(top_k)\n",
        "        top_i = top_i.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next word index with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
        "        \n",
        "        # retrieve that word from the dictionary\n",
        "        word = int_to_vocab[word_i]\n",
        "        predicted.append(word)     \n",
        "        \n",
        "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
        "        current_seq = np.roll(current_seq.cpu(), -1, 1)\n",
        "        current_seq[-1][-1] = word_i\n",
        "    \n",
        "    gen_sentences = ' '.join(predicted)\n",
        "    \n",
        "    # Replace punctuation tokens\n",
        "    for key, token in token_dict.items():\n",
        "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
        "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
        "    gen_sentences = gen_sentences.replace('( ', '(')\n",
        "    \n",
        "    # return all the sentences\n",
        "    return gen_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgx8qAcXKXu_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "06fb4917-acc3-4657-be41-d39984c3d8a6"
      },
      "source": [
        "# run the cell multiple times to get different results!\n",
        "gen_length = 100 # modify the length to your preference\n",
        "prime_word = 'corona' # name for starting the script\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "# def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
        "pad_word = SPECIAL_WORDS['PADDING']\n",
        "generated_script = generate(trained_rnn, vocab_to_int[prime_word], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
        "print(generated_script)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corona. m. est\n",
            "** the dollar slipped 0. 2 percent to 3. 5 percent.\n",
            "in the past decade, the dow jones industrial average was up 0. 2 percent at $1, 326. 00 a barrel.\n",
            "a weaker u. s. dollar slipped from a basket of major currencies in 2018.\n",
            "the dow jones industrial average index was down 0. 3 percent to the lowest level in the past five months,  the official said.\n",
            "the ministry is not immediately available to comment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm9WwDDBDgFh",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis on Stock Data\n",
        "\n",
        "Sentiment analysis on stock data can be added to one's advantage. If you look for an extreme example of how social media influences stock market, take a look at Kylie Jenners tweet about Snapchat. <br>\n",
        "\n",
        "<img src=\"https://github.com/purvasingh96/Talking-points-global-hackathon/blob/master/images/kylie_tweet.png?raw=1\"></img>\n",
        "\n",
        "Shortly after the message was posted online, the price of Snap, Snapchat parent company, fell by 8.5 percent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "091OJ7rxKqWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# read data from text files\n",
        "with open('reviews.txt', 'r') as f:\n",
        "    reviews = f.read()\n",
        "with open('labels.txt', 'r') as f:\n",
        "    labels = f.read()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIb4h6WwDcWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "daede9c9-0046-4b31-a230-af67c3c797b3"
      },
      "source": [
        "print(reviews[:2000])\n",
        "print()\n",
        "print(labels[:20])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
            "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n",
            "homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if y\n",
            "\n",
            "positive\n",
            "negative\n",
            "po\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mfil9uX4Dm9H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8098c502-6c33-4b11-f13e-b0c4dfdb89be"
      },
      "source": [
        "from string import punctuation\n",
        "\n",
        "print(punctuation)\n",
        "\n",
        "# get rid of punctuation\n",
        "reviews = reviews.lower() # lowercase, standardize\n",
        "all_text = ''.join([c for c in reviews if c not in punctuation])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL-Fm_ADDoam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split by new lines and spaces\n",
        "reviews_split = all_text.split('\\n')\n",
        "all_text = ' '.join(reviews_split)\n",
        "\n",
        "# create a list of words\n",
        "words = all_text.split()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4GCW5ykDomJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "4aee21f8-d6b0-419e-f759-6b8df3a068bc"
      },
      "source": [
        "words[:30]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bromwell',\n",
              " 'high',\n",
              " 'is',\n",
              " 'a',\n",
              " 'cartoon',\n",
              " 'comedy',\n",
              " 'it',\n",
              " 'ran',\n",
              " 'at',\n",
              " 'the',\n",
              " 'same',\n",
              " 'time',\n",
              " 'as',\n",
              " 'some',\n",
              " 'other',\n",
              " 'programs',\n",
              " 'about',\n",
              " 'school',\n",
              " 'life',\n",
              " 'such',\n",
              " 'as',\n",
              " 'teachers',\n",
              " 'my',\n",
              " 'years',\n",
              " 'in',\n",
              " 'the',\n",
              " 'teaching',\n",
              " 'profession',\n",
              " 'lead',\n",
              " 'me']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_h43eB7Dopd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word:ii for ii, word in enumerate(vocab, 1)}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq9P1yLiDotp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "583eb661-e3f2-4262-c7ce-006169be0d97"
      },
      "source": [
        "reviews_int = []\n",
        "'''\n",
        "reviews_split contains multiple reviews \n",
        "reviews_int will be 2-D array\n",
        "'''\n",
        "for review in reviews_split:\n",
        "  reviews_int.append([vocab_to_int[word] for word in review.split()])\n",
        "print(len(vocab_to_int))\n",
        "print(reviews_int[:10])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74072\n",
            "[[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23], [63, 4, 3, 125, 36, 47, 7472, 1395, 16, 3, 4181, 505, 45, 17, 3, 622, 134, 12, 6, 3, 1279, 457, 4, 1721, 207, 3, 10624, 7373, 300, 6, 667, 83, 35, 2116, 1086, 2989, 34, 1, 898, 46417, 4, 8, 13, 5096, 464, 8, 2656, 1721, 1, 221, 57, 17, 58, 794, 1297, 832, 228, 8, 43, 98, 123, 1469, 59, 147, 38, 1, 963, 142, 29, 667, 123, 1, 13584, 410, 61, 94, 1774, 306, 755, 5, 3, 819, 10396, 22, 3, 1724, 635, 8, 13, 128, 73, 21, 233, 102, 17, 49, 50, 617, 34, 682, 85, 28785, 28786, 682, 374, 3341, 11398, 2, 16371, 7946, 51, 29, 108, 3324], [22382, 42, 46418, 15, 706, 17139, 3389, 47, 77, 35, 1819, 16, 154, 19, 114, 3, 1305, 5, 336, 147, 22, 1, 857, 12, 70, 281, 1168, 399, 36, 120, 283, 38, 169, 5, 382, 158, 42, 2269, 16, 1, 541, 90, 78, 102, 4, 1, 3244, 15, 43, 3, 407, 1068, 136, 8055, 44, 182, 140, 15, 3043, 1, 320, 22, 4818, 26224, 346, 5, 3090, 2092, 1, 18839, 17939, 42, 8055, 46, 33, 236, 29, 370, 5, 130, 56, 22, 1, 1928, 7, 7, 19, 48, 46, 21, 70, 344, 3, 2099, 5, 408, 22, 1, 1928, 16, 3, 3119, 205, 1, 28787, 21, 281, 68, 38, 3, 339, 1, 700, 715, 3, 3818, 1229, 22, 1, 1491, 3, 1197, 2, 283, 21, 281, 2435, 5, 66, 48, 8, 13, 39, 5, 29, 3244, 12, 6, 21026, 11723, 13, 2015, 7, 7, 3687, 2818, 36, 4147, 36, 374, 15, 11723, 296, 3, 996, 125, 36, 47, 283, 9, 1, 176, 363, 6893, 5, 94, 3, 2099, 17, 3, 4976, 2932, 14557, 19870, 5, 66, 46, 25, 51, 408, 9, 1, 1928, 16, 3236, 490, 205, 1, 28787, 46, 11723, 2845, 25, 51, 80, 48, 25, 483, 17, 3, 682, 1148, 4, 228, 52, 4461, 1, 2099, 13, 22, 118, 11723, 6, 1347, 22, 1, 857, 17, 3, 18840, 22, 27, 3873, 5, 10167, 27, 174, 829, 118, 25, 51, 23, 1456, 123, 1, 6451, 25, 13, 344, 1, 13585, 28788, 34, 3, 32300, 101, 8, 13, 391, 22, 27, 11724, 118, 11723, 874, 81, 103, 577, 3, 240, 34, 1, 393, 4, 4653, 16372, 1816, 3737, 35, 1200, 3103, 36, 188, 4048, 160, 2284, 41, 339, 2, 41, 8809, 6793, 1984, 4313, 2, 28789, 8810, 2457, 36, 26, 453, 338, 5, 1, 1928, 33, 155, 4219, 11723, 215, 23, 25, 13, 24, 338, 5, 4421, 5903, 28790, 39, 25, 281, 120, 54, 111, 996, 118, 8, 13, 534, 42, 2718, 501, 42, 29, 547, 7, 7, 136, 1, 115, 2003, 198, 4653, 2, 11723, 285, 23, 1644, 5, 112, 10, 254, 110, 4354, 5, 29, 30, 4, 3687, 2818, 15686, 107, 118, 2523, 5, 111, 3, 207, 8, 286, 3, 4220, 488, 1060, 5, 27, 2730, 158, 140, 15, 7473, 11399, 184, 4539, 42, 18841, 16, 1, 541, 5, 121, 48, 8, 13, 39, 255, 141, 4504, 160, 2284, 8, 1, 370, 245, 42, 22, 1, 81, 495, 228, 3, 372, 2099, 39, 31, 996, 78, 80, 54, 33, 89, 23, 122, 48, 5, 80, 17, 67, 273, 277, 33, 142, 200, 8, 5, 1, 3244, 303, 4, 757, 8, 39, 17140, 273, 7, 7, 42, 277, 11, 20, 79, 5853, 21, 5, 336, 400], [4505, 505, 15, 3, 3342, 162, 8312, 1652, 6, 4819, 56, 17, 4504, 5616, 140, 11725, 5, 996, 4919, 2933, 4462, 566, 1201, 36, 6, 1518, 96, 3, 744, 4, 26225, 13, 5, 27, 3461, 9, 10625, 4, 8, 111, 3013, 5, 1, 1027, 15, 3, 4390, 82, 22, 2049, 6, 4462, 538, 2764, 7073, 37443, 41, 463, 1, 8312, 46419, 302, 123, 15, 4221, 19, 1667, 922, 1, 1652, 6, 6129, 19871, 34, 1, 980, 1751, 22383, 646, 24104, 27, 106, 11726, 13, 14045, 15097, 17940, 2457, 466, 21027, 36, 3266, 1, 6365, 1020, 45, 17, 2695, 2499, 33, 1305, 5, 2076, 1, 4504, 11727, 1493, 22, 3, 21028, 1652, 3196, 22, 35, 4314, 1067, 19, 136, 228, 27, 4654, 22383, 217, 1906, 35, 3216, 17141, 9, 1, 4148, 1961, 1110, 4, 1, 1652, 5617, 8, 6524, 83, 1, 1958, 118, 8, 8056, 5, 1, 1301, 204, 3985, 9, 1, 641, 4, 1, 32301, 5854, 17, 922, 9, 342, 6200, 1047, 28791, 9, 255, 15687, 119, 1985, 123, 259, 1, 695, 12046, 16, 1, 4820, 13, 15, 33, 12397, 336, 17, 57, 688, 608, 45, 7, 7, 82, 560, 458, 1, 1056, 271, 28792, 4505, 11, 330, 736, 5, 1, 6702, 557, 1661, 689, 4505, 14, 516, 34, 1435, 9121, 136, 281, 173, 39, 8, 13, 8313, 10, 51, 23, 133, 4505, 6, 100, 428, 4, 1527, 349, 8, 6, 440, 256, 24, 2686, 16, 1, 204, 987, 45, 4, 1, 287, 4505, 107, 10, 28, 108, 37, 227, 10, 165, 418, 11, 30, 1, 117, 43, 8, 47, 60, 1621, 112, 4, 1, 287, 17, 3, 324, 1667, 922, 6129, 32302, 93, 1, 6524, 161, 23, 25, 66, 1, 3216, 17141, 6201, 4, 1, 277, 1, 1154, 70, 264, 5, 1638, 1, 201, 4505, 17, 159, 1043, 1661, 494, 4, 1, 791, 1, 32303, 1115, 18842, 6, 118, 8, 2656, 363, 1, 130, 17, 3, 5245, 6287, 4391, 147, 2582, 982, 343, 37444, 54, 1, 922, 1106, 45, 42, 10878, 15, 1, 24105, 42, 46, 100, 4, 1, 3529, 26, 3013, 8, 13, 3, 531, 322, 12, 97, 28, 91, 16, 3, 85, 116, 1661, 494, 19, 76, 6794, 104, 13, 739, 410, 12047, 265, 1298, 3, 146, 574, 4, 2327, 42, 817, 42, 1054, 800, 11, 6, 3, 1026, 1400, 136, 1, 246, 12767, 112, 918, 30, 2144, 16, 1006, 229, 24, 12, 74, 559, 101, 1, 1652, 8056, 40, 13, 24, 15, 74, 8811, 15, 10, 195, 40, 142, 28, 77, 59, 54, 1, 2914, 409, 562, 182, 89, 23, 1236, 56, 12, 74, 17, 3, 170, 648, 4, 653, 5097, 10626, 1518, 44, 19, 40, 13, 43, 141, 1867, 127, 706, 4015, 15, 1, 37445, 12048, 3444, 865, 37446, 6, 144, 19, 64, 211, 3, 368, 4, 137, 1176, 59, 549, 231, 16373, 5, 43, 167, 3738, 9, 1, 958, 7, 7, 1, 339, 366, 2220, 310, 4, 4505, 506, 229, 136, 1, 178, 242, 2033, 747, 35, 1685, 522, 4, 908, 577, 3, 162, 622, 878, 702, 109, 52, 137, 17, 706, 4015, 15, 37446, 2123, 5, 2077, 45, 104, 13, 1184, 2196, 137, 1, 3766, 42, 159, 368, 4, 340, 2306, 577, 1, 32304, 136, 10, 61, 39, 5, 66, 11, 1685, 908, 10, 239, 24, 249, 10, 97, 848, 145, 3, 733, 287, 522, 584, 4, 4505, 15, 853, 1, 20, 47, 1936, 889, 17, 517, 7818, 7374, 1571, 2800, 10, 79, 133, 58, 52, 81, 73, 1, 2819, 1652, 2117, 298, 696, 23, 85, 343, 364, 17, 1, 81, 106, 4505, 2256, 11, 302, 3076, 4, 269, 9, 1, 10627, 1315, 13, 2280, 4, 879, 256, 10, 51, 102, 4, 760, 4, 429, 107, 73, 11, 37, 10, 10879, 12, 13, 3, 116, 2458, 1, 202, 137, 26, 3, 116, 739, 464, 1, 1040, 6, 539, 24, 74, 2285, 42, 1054, 6, 4821, 62, 6, 3, 879, 15, 10, 10879, 11, 97, 28, 77, 3, 183, 50, 20, 46, 91, 2846, 7, 7, 1, 360, 1205, 26, 2626, 46, 163, 2068, 1, 113, 215, 23, 85, 106, 57, 708, 2186, 669, 3949, 47, 301, 234, 8, 14, 3, 1293, 5, 318, 9, 11, 30, 57, 708, 2186, 566, 1201, 268, 153, 11118, 82, 30, 57, 708, 2186, 743, 1968, 268, 1793, 136, 2642, 1331, 743, 6, 344, 116, 5, 80, 40, 26, 934, 4, 81, 1061, 1562, 5, 167, 45, 16, 98, 7, 7, 4505, 6, 1, 90, 1661, 21029, 4, 1, 287, 4505, 107, 37, 227, 10, 418, 1, 989, 485, 8, 59, 46, 33, 70, 3, 223, 694, 1, 360, 1889, 451, 151, 23, 336, 150, 3, 20, 44, 3, 15688, 1652, 43, 1587, 23, 29, 11, 354, 42, 12047, 1457, 34, 1, 22384, 4505], [520, 119, 113, 34, 16372, 1816, 3737, 117, 885, 21030, 721, 10, 28, 124, 108, 2, 115, 137, 9, 1623, 7691, 26, 330, 5, 589, 1, 6130, 22, 386, 6, 3, 349, 15, 50, 15, 231, 9, 7473, 11399, 1, 191, 22, 8966, 6, 82, 880, 101, 111, 3584, 4, 111, 3, 28793, 3445, 45, 27, 1324, 2, 111, 12398, 1, 2360, 4, 28788, 11723, 24106, 32305, 10, 143, 3, 2360, 25, 549, 287, 164, 697, 4016, 19870, 3, 502, 38, 1, 299, 2643, 6525, 121, 6, 759, 127, 98, 15, 3, 1135, 5098, 36, 483, 5, 4182, 1, 6608, 27, 104, 6, 52, 10397, 73, 630, 1, 1519, 134, 2, 1, 134, 118, 1, 3244, 17142, 3, 14046, 2100, 26, 31, 57, 2179, 167, 16, 1, 2951, 134, 2, 1, 106, 192, 15689, 975, 30, 24107, 11, 18, 211, 128, 253, 57, 10, 66, 8, 62, 6, 179, 395], [11, 20, 3637, 141, 10, 422, 23, 272, 60, 4355, 22, 32, 84, 3286, 22, 1, 172, 4, 1, 952, 507, 11, 4977, 5361, 5, 574, 4, 1155, 54, 53, 5304, 1, 261, 17, 41, 952, 125, 59, 1, 711, 137, 379, 626, 15, 111, 1509, 1, 156, 32, 292, 8, 97, 55, 72, 28, 77, 1, 157, 36, 37447, 48, 25, 871, 38, 1, 156, 10, 43, 89, 23, 122, 7, 7, 19, 97, 8, 28, 77, 1, 860, 43, 605, 36, 14, 1, 8812, 9, 115, 17, 25, 460, 52, 15098, 4, 27, 28794, 1937, 2, 3688, 2, 1092, 4, 309, 2, 27, 6131, 6056, 73, 4, 1702, 42, 231, 326, 25, 114, 2312, 71, 25, 14, 9, 115, 17, 1, 2484, 7, 7, 10, 14, 672, 9, 11, 18, 19, 89, 23, 836, 8, 14, 2270, 16, 35, 708, 37, 1857, 16, 610], [11, 6, 692, 1, 90, 2156, 20, 11728, 1, 2818, 5195, 249, 92, 3006, 8, 126, 24, 200, 3, 802, 634, 4, 22382, 1001, 133, 87, 3530, 3343, 509, 3, 802, 634, 4, 16374, 5096, 42, 2552, 509, 3, 802, 634, 4, 8967, 21, 3709, 109, 4, 1, 623, 787, 1010, 19, 131, 11, 20, 6, 55, 3178, 9, 3, 95, 109, 1258, 26, 24, 2, 5, 1572, 12, 123, 9, 3, 63, 44, 49, 4, 1, 90, 11729, 22385, 1039, 4, 875, 6, 365, 1132, 92, 24, 1, 3446, 607, 19, 92, 24, 582, 343, 60, 64, 3267, 6, 12, 2818, 142, 28, 177, 279, 326, 9, 1, 475, 10, 115, 3687, 15, 3, 157, 2, 533, 24, 37, 74, 15, 3, 475], [786, 295, 10, 122, 11, 6, 419, 5, 29, 35, 482, 20, 19, 1281, 33, 142, 28, 2657, 45, 1840, 32, 1, 2778, 37, 78, 97, 2436, 67, 3950, 45, 2, 24, 105, 256, 1, 134, 1571, 2, 12399, 451, 14, 319, 11, 63, 6, 98, 1321, 5, 105, 1, 3767, 4, 3, 472, 1381, 14, 1736, 1, 46420, 648, 70, 98, 194, 87, 194, 51, 21, 105, 106, 78, 43, 1238, 40, 2, 642, 257, 54, 1, 410, 6, 106, 78, 5246, 10, 65, 68, 3, 250, 57, 43, 390, 145, 11, 20, 1, 351, 70, 319, 19, 87, 74, 4, 12, 454, 14558, 3585, 529, 51, 21, 191, 1, 64, 152, 10, 418, 14, 7074, 14559, 2, 41, 737, 2670, 2, 1073, 134, 881, 11, 14, 3, 7474, 4, 4392, 2, 10, 143, 58, 331, 1311, 27, 343, 10, 102, 251, 36, 549, 33, 499, 620, 4, 11, 6, 72, 3148], [11, 6, 24, 1, 779, 3687, 2818, 20, 8, 14, 74, 325, 2730, 73, 90, 4, 27, 99, 2, 165, 68, 3, 112, 12, 14, 28795, 2779, 1816, 3737, 91, 1, 18, 53, 6, 140, 3, 759, 458, 1124, 507, 40, 70, 49, 381, 12, 97, 28, 77, 6526, 45, 3, 223, 52, 2, 49, 137, 12, 97, 238, 28, 77, 584, 5, 94, 1, 654, 5, 80, 37, 19, 31, 9, 31, 11, 6, 288, 1, 1783, 5, 831, 2, 66, 8, 1, 113, 14, 50, 442, 2818, 309, 120, 3, 50, 289, 205, 27, 7576, 1362, 5, 2514, 5, 1, 300, 173, 3737, 14, 1, 117, 270, 9, 1, 18, 19, 37448, 2, 6793, 197, 252, 67, 521, 72], [54, 10, 14, 116, 60, 798, 552, 71, 364, 5, 1, 730, 5, 66, 8057, 8, 14, 30, 4, 109, 99, 10, 293, 17, 60, 798, 19, 11, 14, 1, 64, 30, 69, 2500, 45, 4, 234, 93, 10, 68, 114, 108, 8057, 363, 43, 1009, 2, 10, 97, 28, 1431, 45, 1, 357, 4, 60, 110, 205, 8, 48, 3, 1929, 10880, 2, 2124, 354, 412, 4, 13, 6609, 2, 2974, 5148, 2125, 1366, 6, 30, 4, 60, 502, 876, 19, 8057, 6, 34, 227, 1, 247, 412, 4, 582, 4, 27, 599, 9, 1, 13586, 396, 4, 14047, 16375, 1366, 403, 178, 3, 454, 21031, 8968, 2601, 9, 5, 1, 450, 4, 3, 212, 11730, 34, 1, 1946, 3986, 2145, 34, 4048, 22386, 599, 115, 683, 115, 46421, 824, 1, 20, 4607, 47, 58, 680, 2113, 58, 224, 2, 6, 14048, 9, 9505, 6795, 11, 20, 396, 51, 29, 117, 4741, 15, 12768, 9, 849, 757, 35, 24108, 4149, 4, 410, 5, 14049, 3, 52, 9924, 1111, 4, 1191, 2, 854, 19, 2125, 1366, 6, 58, 14047, 4392, 1, 20, 6, 2124, 539, 2, 739, 19, 705, 12, 10, 328, 68, 58, 2003, 17, 42, 2467, 16, 100, 4, 1, 103, 303, 10, 416, 64, 5974, 16, 11, 5149, 4, 17941, 6527, 28796, 15690, 28797, 9, 3, 11731, 2644, 16, 2016, 8314, 3, 4078, 4, 22387, 37449, 2, 8058, 37450, 28798, 1, 63, 19872, 39, 3, 7375, 9, 1, 654, 295, 2437, 9, 3844, 16376, 2, 10881, 1074, 198, 10882, 295, 6, 407, 2, 2485, 1645, 5, 168, 451, 42, 1879, 42, 824, 2, 8, 43, 266, 22, 2, 22, 5, 1, 210, 118, 21, 43, 181, 5, 3606, 31, 4, 96, 8, 13, 114, 44, 3429, 8, 13, 64, 44, 9696, 13587, 13154, 8, 6, 163, 52, 73, 3, 1946, 448, 614, 5, 35, 1539, 705, 1, 300, 13, 1230, 5, 3739, 2125, 1366, 2438, 5, 94, 103, 37, 9320, 9, 525, 69, 230, 313, 45, 2, 16, 12, 284, 10, 254, 11, 18, 2124, 524, 5050, 2, 17942, 13588, 10, 66, 48, 25, 14, 169, 16, 19, 27, 13155, 22, 8485, 27, 729, 145, 24109, 15691, 2, 8178, 20, 3325, 32306, 8, 491, 1, 210, 4, 7075, 10, 535, 380, 11, 30, 46, 21, 155, 537, 3, 116, 98, 631, 2, 355, 141, 5, 2988, 21, 4, 333, 881, 278, 13, 43, 3874, 11, 20, 114, 563]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR4J17duDow3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "08af0a77-fab3-4ae2-8d2e-2db8931a337e"
      },
      "source": [
        "# stats about vocabulary\n",
        "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
        "print()\n",
        "\n",
        "# print tokens in first review\n",
        "print('Tokenized review: \\n', reviews_int[:1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words:  74072\n",
            "\n",
            "Tokenized review: \n",
            " [[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnWkHlyYDozz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "460a40c4-dffc-4c0d-fd6c-191c96c775b0"
      },
      "source": [
        "labels_split = labels.split('\\n')\n",
        "labels_to_int = np.array([1 if label=='positive' else 0 for label in labels_split])\n",
        "\n",
        "zero_length_reviews = Counter([len(x) for x in reviews_int])\n",
        "print(max(zero_length_reviews))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdBviRlyDo2s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "38a58635-5da1-4bf3-c8f4-8361bda39862"
      },
      "source": [
        "# outlier review stats\n",
        "review_lens = Counter([len(x) for x in reviews_int])\n",
        "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
        "print(\"Maximum review length: {}\".format(max(review_lens)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length reviews: 1\n",
            "Maximum review length: 2514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENcAXvlPDo50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8d2c574c-ba23-4c38-8848-0fe94401c946"
      },
      "source": [
        "print('Number of reviews before removing outliers: ', len(reviews_int))\n",
        "\n",
        "## remove any reviews/labels with zero length from the reviews_ints list.\n",
        "\n",
        "non_zero_idx = [ii for ii, review in enumerate(reviews_int) if len(review)!=0]\n",
        "reviews_int = [reviews_int[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([labels_to_int[ii] for ii in non_zero_idx])\n",
        "\n",
        "\n",
        "print('Number of reviews after removing outliers: ', len(reviews_int))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews before removing outliers:  25001\n",
            "Number of reviews after removing outliers:  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFs5r83QDo8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_features(reviews_int, seq_length):\n",
        "  features = np.zeros((len(reviews_int), seq_length), dtype=int)\n",
        "  for i, row in enumerate(reviews_int):\n",
        "    features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "  \n",
        "  return features"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpEfMn6NDpFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test your implementation!\n",
        "\n",
        "seq_length = 200\n",
        "features = pad_features(reviews_int, seq_length)\n",
        "print(features[:30, :10])\n",
        "\n",
        "## test statements - do not change - ##\n",
        "assert len(features)==len(reviews_int), \"Your features should have as many rows as reviews.\"\n",
        "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
        "\n",
        "# print first 10 values of the first 30 batches \n",
        "print(features[:30,:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkQVICT-DpIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "split_idx = int(len(features)*split_frac)\n",
        "\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADJPzqa2DpLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm6W-5THEHuD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "20c5a683-73b4-4f9c-f498-c751ba23c243"
      },
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[   10,   254,   131,  ...,    17,    88,     2],\n",
            "        [    0,     0,     0,  ...,     2,   425,  1470],\n",
            "        [   10,    43,  2368,  ...,    12,    40,     6],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ...,   115,    17,   273],\n",
            "        [   10, 17710,    11,  ...,   277,     1,  1020],\n",
            "        [    0,     0,     0,  ...,    33,    70,  1553]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
            "        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
            "        0, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R37SpjO5EH4r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2a6316c-34cc-456b-e423-315b684f640e"
      },
      "source": [
        "# First checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak0Tt-ZfEH8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "  def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "    super(SentimentRNN, self).__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "\n",
        "    self.dropout = nn.Dropout(0.3)\n",
        "    self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    self.sig = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    batch_size = x.size(0)\n",
        "    x = x.long()\n",
        "    embeds = self.embedding(x)\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "    out = self.dropout(lstm_out)\n",
        "    out = self.fc(out)\n",
        "    sig_out = self.sig(out)\n",
        "\n",
        "    sig_out = sig_out.view(batch_size, -1)\n",
        "    sig_out = sig_out[:, -1]\n",
        "\n",
        "    return sig_out, hidden\n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    if(train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(), \n",
        "                weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim), \n",
        "                weight,new(self.n_layers, batch_size, self.hidden_dim))\n",
        "      \n",
        "    return hidden"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5pvaU1hEH_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d569dc79-9235-475b-bba8-addcb2624a99"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(74073, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY9m5shaEV94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzmqEmmSEWe4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "bafa377f-8762-49f3-861e-8d536f6d0c6b"
      },
      "source": [
        "epochs = 4\n",
        "counter = 0 \n",
        "print_every = 100\n",
        "clip = 5\n",
        "if(train_on_gpu):\n",
        "  net.cuda()\n",
        "\n",
        "net.train()\n",
        "for e in range(epochs):\n",
        "  h = net.init_hidden(batch_size)\n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1\n",
        "    if(train_on_gpu):\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    h = tuple([each.data for each in h])\n",
        "    net.zero_grad()\n",
        "    output, h = net(inputs, h)\n",
        "    loss = criterion(output.squeeze(), labels.float())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm(net.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if counter % print_every == 0:\n",
        "      val_h = net.init_hidden(batch_size)\n",
        "      val_losses = []\n",
        "      net.eval()\n",
        "      for inputs, labels in valid_loader:\n",
        "        val_h = tuple([each.data for each in val_h])\n",
        "        if(train_on_gpu):\n",
        "          inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        output, val_h = net(inputs, val_h)\n",
        "        val_loss = criterion(output.squeeze(), labels.float())\n",
        "        val_losses.append(val_loss.item())\n",
        "      net.train()\n",
        "      print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/4... Step: 100... Loss: 0.693711... Val Loss: 0.653396\n",
            "Epoch: 1/4... Step: 200... Loss: 0.544701... Val Loss: 0.571027\n",
            "Epoch: 1/4... Step: 300... Loss: 0.562405... Val Loss: 0.557840\n",
            "Epoch: 1/4... Step: 400... Loss: 0.646383... Val Loss: 0.633329\n",
            "Epoch: 2/4... Step: 500... Loss: 0.678114... Val Loss: 0.709534\n",
            "Epoch: 2/4... Step: 600... Loss: 0.566871... Val Loss: 0.530177\n",
            "Epoch: 2/4... Step: 700... Loss: 0.494823... Val Loss: 0.530239\n",
            "Epoch: 2/4... Step: 800... Loss: 0.345891... Val Loss: 0.454900\n",
            "Epoch: 3/4... Step: 900... Loss: 0.307721... Val Loss: 0.481526\n",
            "Epoch: 3/4... Step: 1000... Loss: 0.329811... Val Loss: 0.469423\n",
            "Epoch: 3/4... Step: 1100... Loss: 0.299512... Val Loss: 0.490447\n",
            "Epoch: 3/4... Step: 1200... Loss: 0.238392... Val Loss: 0.461704\n",
            "Epoch: 4/4... Step: 1300... Loss: 0.311766... Val Loss: 0.491983\n",
            "Epoch: 4/4... Step: 1400... Loss: 0.336497... Val Loss: 0.480622\n",
            "Epoch: 4/4... Step: 1500... Loss: 0.345086... Val Loss: 0.544988\n",
            "Epoch: 4/4... Step: 1600... Loss: 0.177175... Val Loss: 0.495150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVeU2Nr5EWMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "18dabe36-661f-4e04-a78d-64a02d53bb30"
      },
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.488\n",
            "Test accuracy: 0.812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYITRozZEWKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from string import punctuation\n",
        "\n",
        "def tokenize_movie_review(test_review):\n",
        "  test_review = test_review.lower()\n",
        "  test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "  test_words = test_text.split()\n",
        "  test_ints = []\n",
        "  test_ints.append([vocab_to_int[word] for word in test_words])\n",
        "  return test_ints"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVWH6UeaEICD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "41eccb91-3cad-4b30-8bbe-57f0e7ac779d"
      },
      "source": [
        "test_review_neg = \"It was a very bad movie. Terrible acting.\"\n",
        "tokenized_review = tokenize_movie_review(test_review_neg)\n",
        "print(tokenize_movie_review(test_review_neg))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8, 14, 3, 55, 76, 18, 388, 113]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lbGUaYbElHG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e5425864-6704-43af-8263-8006d6ffa553"
      },
      "source": [
        "seq_length = 200\n",
        "features = pad_features(tokenized_review, seq_length)\n",
        "print(features)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   8  14   3  55  76  18\n",
            "  388 113]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTNB-HOiElSB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5005710a-1735-4f87-8c52-35aa0ec4262a"
      },
      "source": [
        "feature_tensor = torch.from_numpy(features)\n",
        "print(feature_tensor.size(0))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtMoZgVLEldD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, test_review, seq_length=200):\n",
        "  net.eval()\n",
        "  test_ints = tokenize_movie_review(test_review)\n",
        "  seq_length=seq_length\n",
        "  features = pad_features(test_ints, seq_length)\n",
        "  feature_tensor = torch.from_numpy(features)\n",
        "  batch_size = feature_tensor.size(0)\n",
        "  h = net.init_hidden(batch_size)\n",
        "  if(train_on_gpu):\n",
        "    feature_tensor=feature_tensor.cuda()\n",
        "  output, h = net(feature_tensor, h)\n",
        "  pred = torch.round(output.squeeze())\n",
        "  print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
        "  if(pred.item()==1):\n",
        "    print(\"Positive\")\n",
        "  else:\n",
        "    print(\"Negative\")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLfp5m0gEqeh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "13de7560-b1c3-41fa-a544-8c6f6c00a2ea"
      },
      "source": [
        "# negative test review\n",
        "test_review_neg = 'Stocks are going down as new corona virus cases surge'\n",
        "seq_length=200 # good to use the length that was trained on\n",
        "\n",
        "predict(net, test_review_neg, seq_length)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction value, pre-rounding: 0.359334\n",
            "Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZflNB7-9EqjA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5e684fa7-b1ed-4dd7-c526-34155bbf1f3c"
      },
      "source": [
        "# negative test review\n",
        "test_review_neg = 'Stock market is booming and growing post corona pandemic.'\n",
        "seq_length=200 # good to use the length that was trained on\n",
        "\n",
        "predict(net, test_review_neg, seq_length)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction value, pre-rounding: 0.717415\n",
            "Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsnUyAqouEp6",
        "colab_type": "text"
      },
      "source": [
        "# Converting Result into CSV File\n",
        "\n",
        "Finally, we are converting the generated talking points and the corresponding sentiment analysis into a csv file which can be further called from an API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04QVNUP5Eqnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNcY-9LW902j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generated_talking_points = x.split('.')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU2GCptIEqsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "with open('talking_agenda.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"SN\", \"Talking Point\", \"Sentiment Prediction\"])\n",
        "    for i, points in enumerate(generated_talking_points):\n",
        "      sentiment_predicted = predict(net, points, seq_length)\n",
        "      writer.writerow([i, points, sentiment_predicted])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXlSw_2S-kMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}